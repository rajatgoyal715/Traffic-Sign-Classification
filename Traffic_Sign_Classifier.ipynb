{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First, we use 'pickled' files to load the traffic sign data!\n",
    "\n",
    "Then, we categorize the training and testing set's features (basis on which we classify images)\n",
    "and labels (basis on which we categorise the images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "# Say we have a dictionary of images and a list of labels - we can append the labels to the corresponding images\n",
    "# by using pickled format in python. In other words, we can say it's used for Serialization \n",
    "# (process of translating data structures or object state into a format that can be stored)\n",
    "# In other words, the 'pickled' format can be stored and our images and labels \n",
    "\n",
    "training_file = 'traffic-signs-data/train.p'\n",
    "# validation_file=?\n",
    "testing_file = 'traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "# with open(validation_file, mode='rb') as f:\n",
    "#     valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "x_train, y_train = train['features'], train['labels']\n",
    "# X_valid, y_valid = valid['features'], valid['labels']\n",
    "x_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# In this block of code, we simply use the python function `len()` to get the length of various subsets of data\n",
    "# This is an essential step of data science and we do it to better understand the data we have.\n",
    "\n",
    "\n",
    "# Number of training samples.\n",
    "n_train = len(x_train)\n",
    "\n",
    "# Number of testing samples.\n",
    "n_test = len(x_test)\n",
    "\n",
    "# The variable `shape` is defined on the dataset. This returns the dimensions of the first image:\n",
    "image_shape = x_train[0].shape\n",
    "\n",
    "# We use the `unique()` function of numpy with `y_train` parameters and nest it in the len() function\n",
    "# in order to find out the length of unique labels in our training set.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n",
    "\n",
    "This is also an extremely important part of data science. We plot graphically and view random samples of the \n",
    "initial data we have, so we can better understand what is required to clean/augment it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We use the following block of code to generate a random index, ranging upto the set of training data\n",
    "# Then, we plot the image to have a better understanding of the type of data we are working with.\n",
    "\n",
    "index = random.randint(0, len(x_train))\n",
    "image = x_train[index]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(image)\n",
    "\n",
    "print(\"Class :\", y_train[index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a list and plot the various different types of images we have (images of each class)\n",
    "# Note: We have 43 classes of signs.\n",
    "\n",
    "num_of_samples=[]\n",
    "plt.figure(figsize=(12, 16.5))\n",
    "for i in range(0, n_classes):\n",
    "    plt.subplot(11, 4, i+1)\n",
    "    x_selected = x_train[y_train == i]\n",
    "    plt.imshow(x_selected[0, :, :, :]) # Draw the first image of each class\n",
    "    plt.title(i)\n",
    "    plt.axis('off')\n",
    "    num_of_samples.append(len(x_selected))\n",
    "plt.show()\n",
    "\n",
    "# Plot number of images per class\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(0, n_classes), num_of_samples)\n",
    "plt.title(\"Distribution of the train dataset\")\n",
    "plt.xlabel(\"Class number\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Min number of images in any one given class =\", min(num_of_samples))\n",
    "print(\"Max number of images in any one given class =\", max(num_of_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `sklearn` package contains a method which allows us to divide our training and validation data in a specific ratio\n",
    "# Here, we chose to divide our data in a 80:20 ratio of training:validation data.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_valid,y_train,y_valid=train_test_split(x_train,y_train,test_size=0.2,random_state=0)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We shuffle the training data by using the algorithm below. This is necessary to ensure that our prediction results\n",
    "# On our validation data aren't just being generated because our data is arranged in a specific way.\n",
    "\n",
    "# Further, we reduce our images which are currently in the RGB color model to grayscale, but retain the shape \n",
    "# ie. (x-y-z axis). This is done as the LeNet architecture which we will use on our data accepts data of the 32x32xC\n",
    "# format. Hence, we must retain the z-axis.\n",
    "\n",
    "x_train_gry=np.sum(x_train/3, axis=3, keepdims=True)\n",
    "x_test_gry=np.sum(x_test/3, axis=3, keepdims=True)\n",
    "x_valid_gry=np.sum(x_valid/3, axis=3, keepdims=True)\n",
    "\n",
    "X_train=x_train_gry\n",
    "X_test=x_test_gry\n",
    "X_valid=x_valid_gry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=(X_train-128)/128\n",
    "X_test=(X_test-128)/128\n",
    "X_valid=(X_valid-128)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def random_translate(img):\n",
    "    rows,cols,_ = img.shape\n",
    "    \n",
    "    # allow translation up to px pixels in x and y directions\n",
    "    px = 2\n",
    "    dx,dy = np.random.randint(-px,px,2)\n",
    "\n",
    "    M = np.float32([[1,0,dx],[0,1,dy]])\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    \n",
    "    dst = dst[:,:,np.newaxis]\n",
    "    \n",
    "    return dst\n",
    "\n",
    "def random_scaling(img):   \n",
    "    rows,cols,_ = img.shape\n",
    "\n",
    "    # transform limits\n",
    "    px = np.random.randint(-2,2)\n",
    "\n",
    "    # ending locations\n",
    "    pts1 = np.float32([[px,px],[rows-px,px],[px,cols-px],[rows-px,cols-px]])\n",
    "\n",
    "    # starting locations (4 corners)\n",
    "    pts2 = np.float32([[0,0],[rows,0],[0,cols],[rows,cols]])\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "    dst = cv2.warpPerspective(img,M,(rows,cols))\n",
    "    \n",
    "    dst = dst[:,:,np.newaxis]\n",
    "    \n",
    "    return dst\n",
    "\n",
    "def random_warp(img):\n",
    "    \n",
    "    rows,cols,_ = img.shape\n",
    "\n",
    "    # random scaling coefficients\n",
    "    rndx = np.random.rand(3) - 0.5\n",
    "    rndx *= cols * 0.06   # this coefficient determines the degree of warping\n",
    "    rndy = np.random.rand(3) - 0.5\n",
    "    rndy *= rows * 0.06\n",
    "\n",
    "    # 3 starting points for transform, 1/4 way from edges\n",
    "    x1 = cols/4\n",
    "    x2 = 3*cols/4\n",
    "    y1 = rows/4\n",
    "    y2 = 3*rows/4\n",
    "\n",
    "    pts1 = np.float32([[y1,x1],\n",
    "                       [y2,x1],\n",
    "                       [y1,x2]])\n",
    "    pts2 = np.float32([[y1+rndy[0],x1+rndx[0]],\n",
    "                       [y2+rndy[1],x1+rndx[1]],\n",
    "                       [y1+rndy[2],x2+rndx[2]]])\n",
    "\n",
    "    M = cv2.getAffineTransform(pts1,pts2)\n",
    "\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    \n",
    "    dst = dst[:,:,np.newaxis]\n",
    "    \n",
    "    return dst\n",
    "\n",
    "def random_brightness(img):\n",
    "    shifted = img + 1.0   # shift to (0,2) range\n",
    "    img_max_value = max(shifted.flatten())\n",
    "    max_coef = 2.0/img_max_value\n",
    "    min_coef = max_coef - 0.1\n",
    "    coef = np.random.uniform(min_coef, max_coef)\n",
    "    dst = shifted * coef - 1.0\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_indices = []\n",
    "output_indices = []\n",
    "\n",
    "for class_n in range(n_classes):\n",
    "    print(class_n, ': ', end='')\n",
    "    class_indices = np.where(y_train == class_n)\n",
    "    n_samples = len(class_indices[0])\n",
    "    if n_samples < 800:\n",
    "        for i in range(800 - n_samples):\n",
    "            input_indices.append(class_indices[0][i%n_samples])\n",
    "            output_indices.append(X_train.shape[0])\n",
    "            new_img = X_train[class_indices[0][i % n_samples]]\n",
    "            new_img = random_translate(random_scaling(random_warp(random_brightness(new_img))))\n",
    "            X_train= np.concatenate((X_train, [new_img]), axis=0)\n",
    "            y_train = np.concatenate((y_train, [class_n]), axis=0)\n",
    "            if i % 50 == 0:\n",
    "                print('|', end='')\n",
    "            elif i % 10 == 0:\n",
    "                print('-',end='')\n",
    "    print('')\n",
    "            \n",
    "print('X, y shapes:', X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** 5x5x1x6 filter ,outputs 28X28X6.\n",
    "\n",
    "**Activation.** Relu Activation.\n",
    "\n",
    "**Pooling.** 2X2 filter with stride 2,outputs 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** 5X5x6x16 filter,outputs 10x10x16\n",
    "\n",
    "**Activation.** Relu Activation.\n",
    "\n",
    "**Pooling.** 2X2 filter with stride 2,outputs 5x5x16.\n",
    "\n",
    "**Layer 3: Convolutional.** 5X5x16x400 filter,outputs 1x1x400\n",
    "\n",
    "**Activation.** Relu Activation.\n",
    "\n",
    "**Flatten.** Concatenation of layer 2 and layer 3 after flattening,outputs 1x800\n",
    "\n",
    "**Dropout.** Dropout with prob 0.5 on training data.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** Gives 43 outputs.\n",
    "\n",
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def traffic_sign(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    weights={\n",
    "        'wc1':tf.Variable(tf.truncated_normal(shape=(5,5,1,6),mean = mu, stddev = sigma)),\n",
    "        'wc2':tf.Variable(tf.truncated_normal(shape=(5,5,6,16),mean = mu, stddev = sigma)),\n",
    "        'wc3':tf.Variable(tf.truncated_normal(shape=(5,5,16,400),mean = mu, stddev = sigma)),\n",
    "        'out':tf.Variable(tf.truncated_normal(shape=(800,43),mean = mu, stddev = sigma))\n",
    "    }\n",
    "    biases={\n",
    "        'bc1':tf.Variable(tf.zeros((6,))),\n",
    "        'bc2':tf.Variable(tf.zeros((16,))),\n",
    "        'bc3':tf.Variable(tf.zeros((400,))),\n",
    "        'out':tf.Variable(tf.zeros((43,)))\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    layer1=tf.nn.conv2d(x,weights['wc1'],strides=[1,1,1,1],padding='VALID')+biases['bc1']\n",
    "    # TODO: Activation.\n",
    "    layer1=tf.nn.relu(layer1)\n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    layer1=tf.nn.max_pool(layer1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    print(layer1.get_shape())\n",
    "    #Dropout\n",
    "    #layer1=tf.nn.dropout(layer1,0.6)\n",
    "    #print(layer1.shape)\n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x16.\n",
    "    layer2=tf.nn.conv2d(layer1,weights['wc2'],strides=[1,1,1,1],padding='VALID')+biases['bc2']\n",
    "    # TODO: Activation.\n",
    "    layer2=tf.nn.relu(layer2)\n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    layer2=tf.nn.max_pool(layer2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    print(layer2.get_shape())\n",
    "    #Dropout\n",
    "    layer3=tf.nn.conv2d(layer2,weights['wc3'],strides=[1,1,1,1],padding='VALID')+biases['bc3']\n",
    "    # TODO: Activation.\n",
    "    layer3=tf.nn.relu(layer3)\n",
    "    print(layer3.get_shape())\n",
    "    layer2_flat=flatten(layer2)\n",
    "    layer3_flat=flatten(layer3)\n",
    "#   \n",
    "    layer4=tf.concat([layer3_flat, layer2_flat], 1)\n",
    "    layer4=tf.nn.dropout(layer4,keep_prob)\n",
    "    \n",
    "    logits=tf.add(tf.matmul(layer4,weights['out']),biases['out'])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "keep_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.0009\n",
    "KEEP_PROB = 0.5\n",
    "logits = traffic_sign(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y,keep_prob:1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y,keep_prob:KEEP_PROB})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "fig, axs = plt.subplots(2,4, figsize=(4, 2))\n",
    "fig.subplots_adjust(hspace = .2, wspace=.001)\n",
    "axs = axs.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images=[]\n",
    "new_labels=[35,14,22,15,37,18]\n",
    "for i, img in enumerate(glob.glob('./new_images/*.jpg')):\n",
    "    image = cv2.imread(img)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    new_images.append(image)\n",
    "new_images=np.asarray(new_images)\n",
    "new_labels=np.asarray(new_labels)\n",
    "\n",
    "for i in range(len(new_images)):\n",
    "#     print(new_images[i].shape)\n",
    "    plt.imshow(new_images[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing and preprocessing new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_images=new_images\n",
    "new_images=np.sum(new_images/3, axis=3, keepdims=True)\n",
    "new_images=(new_images-128)/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(new_images, new_labels)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_logits = tf.nn.softmax(logits)\n",
    "top_k = tf.nn.top_k(softmax_logits, k=3)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph('./lenet.meta')\n",
    "    saver.restore(sess, \"./lenet\")\n",
    "    my_softmax_logits = sess.run(softmax_logits, feed_dict={x: new_images, keep_prob: 1.0})\n",
    "    my_top_k = sess.run(top_k, feed_dict={x: new_images, keep_prob: 1.0})\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(len(new_images),4, figsize=(12, 14))\n",
    "    fig.subplots_adjust(hspace = .4, wspace=.2)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    for i, image in enumerate(my_images):\n",
    "        axs[4*i].axis('off')\n",
    "        axs[4*i].imshow(cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "        axs[4*i].set_title('input')\n",
    "        guess1 = my_top_k[1][i][0]\n",
    "        index1 = np.argwhere(y_valid == guess1)[0]\n",
    "        axs[4*i+1].axis('off')\n",
    "        axs[4*i+1].imshow(X_valid[index1].squeeze(), cmap='gray')\n",
    "        axs[4*i+1].set_title('top guess: {} ({:.0f}%)'.format(guess1, 100*my_top_k[0][i][0]))\n",
    "        guess2 = my_top_k[1][i][1]\n",
    "        index2 = np.argwhere(y_valid == guess2)[0]\n",
    "        axs[4*i+2].axis('off')\n",
    "        axs[4*i+2].imshow(X_valid[index2].squeeze(), cmap='gray')\n",
    "        axs[4*i+2].set_title('2nd guess: {} ({:.0f}%)'.format(guess2, 100*my_top_k[0][i][1]))\n",
    "        guess3 = my_top_k[1][i][2]\n",
    "        index3 = np.argwhere(y_valid == guess3)[0]\n",
    "        axs[4*i+3].axis('off')\n",
    "        axs[4*i+3].imshow(X_valid[index3].squeeze(), cmap='gray')\n",
    "        axs[4*i+3].set_title('3rd guess: {} ({:.0f}%)'.format(guess3, 100*my_top_k[0][i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8,2, figsize=(9, 19))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(len(my_softmax_logits)*2):\n",
    "    if i%2 == 0:\n",
    "        axs[i].axis('off')\n",
    "        axs[i].imshow(cv2.cvtColor(my_images[i//2], cv2.COLOR_BGR2RGB))\n",
    "    else:\n",
    "        axs[i].bar(np.arange(n_classes), my_softmax_logits[(i-1)//2]) \n",
    "        axs[i].set_ylabel('Softmax probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
